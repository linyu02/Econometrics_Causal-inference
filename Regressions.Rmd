---
title: "Regression Models"
output: pdf_document
---
This notebook presents key econometric concepts in regression modeling with direct applications to data science. It demonstrates how to build and interpret bivariate and multivariate regressions, evaluate model assumptions, and apply both homoskedasticity-only and heteroskedasticity-robust standard errorsâ€”emphasizing why robust inference is often the safer choice. The notebook also shows how to formally write null and alternative hypotheses, perform hypothesis testing, and interpret p-values in real-world contexts. Both continuous and categorical variables are modeled, with examples of dummy variables and interaction terms. By combining simulations with applied data analysis, this work highlights my ability to apply regression techniques to real problems, draw valid conclusions, and communicate insights effectivelyâ€”core skills for data science and analytics roles.

# Bivariate Regression:  
A bivariate regression is the simplest form of regression analysis, where you study the relationship between two variables:
- One dependent variable (Y) â†’ the outcome you want to explain or predict.
- One independent variable (X) â†’ the predictor or explanatory variable.

## homoskedasticity-only standard error vs. heteroskedasticity-robust standard error
Homoskedasticity-only standard error has the assumption that $Var(\epsilon|X_{i})=\sigma^2$, where $sigma$ is a constant. 

Heteroskedasticity-robust standard error doesn't rely on this assumption: $Var(\epsilon|X_{i})=\sigma^2$ and $sigma$ is a constant. Instead, it provides a more consistent estimate of the standard errors. It is always safer to use the robust standard error so we can get better confidence intervals, T-statistics. 

Analogy: Predicting the price of house based on size, homoskedasticity-only standard error would say our model has the same level of prediction power for smaller houses as it does for larger mansions, with assumption that unpredictable factors such as seller's motivation is similar for all houses. However, this might not true, we can not guaranteen that all unpreditable factors have same variabilities. ---> Always use robust standard erros unless you have a very strong, specific reason to believe your errors are homoskedastic. 

Warning: !!! The OLS regression is using this assumption: $Var(\epsilon|X_{i})=\sigma^2$, where $sigma$ is a constant.  And lm model in R automatically use the homoskedasticity-only standard error.

```{r}
# Core packages for regression and econometrics
library(AER)        # Access to CASchools and Mortgage datasets
library(lmtest)     # coeftest() for robust regression tests
library(sandwich)   # vcovHC() for heteroskedasticity-robust SEs
library(car)        # linearHypothesis() for testing linear restrictions

# Data handling and visualization
library(ggplot2)    # plotting
library(dplyr)      # data wrangling (if you plan to tidy up results)

```

### Question 1
Generate data from the model
where ğ›½ = 686 and ğ›½ = 0. Here, ğ‘‹ = expenditure and ğ‘› = 420.
$test.score=ğ›½_0 +ğ›½_1*ğ‘‹ +ğ‘¢$, 
â€¢ Generate â€œexpenditureâ€ from ğ‘(5312, 401956). (So $\mu_{expenditure}=5312$)

â€¢ Generate $ u=\sqrt{|expenditure - \mu_{expenditure}}$, where ğ‘~ğ‘(0,1) is independent of â€œexpenditureâ€.

 Suppose that you use the homoskedasticity-only standard error (the one from lm function in R) to construct a 95% confidence interval for $\beta_{1}$ , i.e., $\hat{\beta_{1}}Â± 1.96â‹… ğ‘†ğ¸$ . Use simulations (2000 repetitions) to compute the coverage probability. Use random seed 947.
```{r}
rm(list=ls())
set.seed(947)
n=420
beta0=686
beta1=0
m=1.96

M=matrix(NA,2000,1)
for (j in 1:2000){
  X1=rnorm(n,5312,sqrt(401956))
  Z=rnorm(n,0,1)
  u=sqrt(abs(X1-5312))*Z
  Y=beta0+beta1*X1+u
  X.bar=mean(X1) 
  Y.bar=mean(Y)
  ols.model=lm(Y~X1) # run the linear model 
  result=summary(ols.model)
  SE.wrong=result$coefficients[2,2] # homoskedasticity-only standard error
  beta1.hat=result$coefficients[2,1]
  lower_bound=beta1.hat-m*SE.wrong # construct the confidence intervals
  Upper_bound=beta1.hat+m*SE.wrong
  M[j]=(lower_bound<beta1&Upper_bound>=beta1) # compute the coverage probability
}
mean(M) #0.8345
```
Now use the heteroskedasticity-robust standard error to construct a 95% confidence interval for $\beta_{1}$. Use simulations (20000 repetitions) to compute the coverage probability. Code up the formulas for  $\hat{\beta_{1}}$
1 and $SE(\hat{\beta_{1}})$. Use random seed 947.
```{r}
rm(list=ls())
set.seed(947)
n=420
beta0=686
beta1=0
m=1.96

M=matrix(NA,20000,1)
for (j in 1:20000){
  X1=rnorm(n,5312,sqrt(401956))
  X=X1
  Z=rnorm(n,0,1)
  u=sqrt(abs(X-5312))*Z
  Y=beta0+beta1*X+u
  X.bar=mean(X) 
  Y.bar=mean(Y)
  beta1.hat= sum((X - X.bar)*(Y - Y.bar))/sum((X - X.bar)^2)
  beta0.hat= Y.bar- beta1.hat*X.bar 
  ##########compute heteroskedasticity-robust standard error##########
  # test bata1
  Y.hat=beta0.hat+beta1.hat*X
  u.hat=Y-Y.hat  
  numerator = sum((X-X.bar)^2*u.hat^2)/(n-2)
  denominator = (sum((X-X.bar)^2)/n)^2
  SE.beta1.hat = sqrt((numerator/denominator)*(1/n))
  lower_bound=beta1.hat-m*SE.beta1.hat
  Upper_bound=beta1.hat+m*SE.beta1.hat
  M[j]=(lower_bound<beta1&Upper_bound>=beta1)
}
mean(M) #0.94615
```
As we can see from observe outputs, heteroskedasticity-robust standard error gives us more accurate confidence intervals. Thus, we should Always use robust standard erros unless you have a very strong, specific reason to believe your errors are homoskedastic.


# Multivariate Regressions + Hypothesis testing:

## Continuous variables 

Question 2
Consider the following regression equation:
ğ‘Œ = ğ›½0 + ğ›½1ğ‘‹1 + ğ›½2ğ‘‹1^2 + ğ›½3ğ‘‹1^3 + ğ›½4ğ‘‹2 + ğ‘¢
â€¢ ğ‘Œ is interest
â€¢ ğ‘‹1 is income
â€¢ ğ‘‹2 is â€œselfâ€
Answer the following questions.
(a) Suppose that we interpret the above regression equation as the effect of ğ‘‹1 on interest, controlling for ğ‘‹2. We are interested in whether or not this effect is linear. Write down the null and alternative hypotheses and compute p value for the null hypothesis.

```{r}
load("/Users/linyu/Desktop/Casual_inference\ /Mortgage_NA4.RData")
################# Question a ################
data_M=Mortgage
Y=data_M$interest
X1=data_M$income
X1_2=X1^2
X1_3=X1^3
X2=data_M$self

reg=lm(Y~X1+X1_2+X1_3+X2)
coeftest(reg, vcov. = vcovHC, type="HC1")

# H0: beta_2 = beta_3 = 0
linearHypothesis(reg, c("X1_2=0","X1_3=0"), white.adjust = "hc1")
# p_value=2.2e-16, reject null hypothesis H0: beta_2 = beta_3 = 0
```

(b) Suppose that we interpret the above regression equation as the effect of ğ‘‹1 on interest,controlling for ğ‘‹2. We are interested in whether or not ğ‘‹1 has any effect (linear or nonlinear) on interest. Write down the null and alternative hypotheses and compute p value for the null hypothesis.

```{r}
# H0: beta1 = beta_2 = beta_3 = 0
################# Question b ################
linearHypothesis(reg, c("X1=0","X1_2=0","X1_3=0"), white.adjust = "hc1")
# p_value=2.2e-16, reject null hypothesis H0: beta1 = beta_2 = beta_3 = 0
```

(c) If a self-employed personâ€™s income increases from $45,000 to $46,000 (in other words, ğ‘‹1 changes from 45 to 46), is the corresponding change in the prediction of ğ‘Œ equal to zero? Write down the null and alternative hypotheses and and compute p value for the null hypothesis.

```{r}
################# Question 3 ################
# compute delta_Y= beta1+91*beta2-120*beta3=0
linearHypothesis(reg, c("X1+91*X1_2-120*X1_3=0"), white.adjust = "hc1")
# p_value=2.2e-16
```

### Test if a Linear constraints of the coefficients holds while running the linear model

Question 4: 
Use â€œCASchools.Rdataâ€. Consider the following regression model
ğ‘Œ = ğ›½0 + ğ›½1ğ‘‹1 + ğ›½2ğ‘‹1^2 + ğ›½3ğ‘‹2 + ğ›½4ğ‘‹2^2 + ğ›½5ğ‘‹1ğ‘‹2 + ğ‘¢
â€¢ ğ‘Œ = test.score, ğ‘‹1 = STR, ğ‘‹2 = lunch

Use â€œlinearHypothesisâ€ command in R to test ğ»0: ğ›½1âˆ’ 3ğ›½3 + ğ›½4 = 2 vs ğ»1: ğ›½1âˆ’ 3ğ›½3 +ğ›½4 â‰  2. Write the code that produces the p-value. 
```{r}
# install.packages("AER")  # if needed
library(AER)
library(lmtest)    # for coeftest
library(sandwich)  # for vcovHC
library(car)       # for linearHypothesis

data("CASchools", package = "AER")
df <- CASchools
names(df)

# Create STR (studentâ€“teacher ratio) and test score;
df$STR <- df$students / df$teachers
df$test.score<- (df$math+df$read+df$english)/3

Y   <- df$math
X1  <- df$STR
X2  <- df$lunch
X1_2 <- X1^2
X2_2 <- X2^2

model <- lm(Y ~ X1 + X1_2 + X2 + X2_2 + X1:X2, data = df)
coeftest(model, vcov. = vcovHC, type = "HC1")

# Test linear restriction: X1 - 3*X2 + X2^2 = 2  (with HC1 adjustment)
linearHypothesis(model, c("X1 - 3*X2 + X2_2 = 2"), white.adjust = "hc1")
#p_value=0.2287
```

## Categorical data/ discrete data:

### Dummy variables:
Dummy variables are indicators coded as 0 or 1 to represent categories in regression models: 
-They allow qualitative factors (like gender, region, or class size group) to be included in a regression; 
-The coefficient on a dummy shows how much the dependent variable differs for that group compared to the reference (omitted) category.

Question 5: 
Use â€œCASchools.Rdataâ€. Create a new variable called â€œclass.sizeâ€. This variable can take 4 values:
â€œlowâ€, â€œmedium-lowâ€, â€œmedium-highâ€ and â€œhighâ€ based on the following criteria
â€¢ If STR<17.75, then class.size is â€œlowâ€.
â€¢ If 17.75â‰¤STR< 19.27, then class.size is â€œmedium-lowâ€.
â€¢ If 19.27â‰¤STR< 20.60, then class.size is â€œmedium-highâ€.
â€¢ If STRâ‰¥20.60, then class.size is â€œhighâ€.
Run the regression of using â€œclass.sizeâ€ to predict â€œtest.scoreâ€. Does â€œclass.sizeâ€ predict
â€œtest.scoreâ€? Compute the p-value for testing the relevant hypothesis.
```{r}
#Question4

df$class.size = NA
df$class.size[df$STR<17.75]="low"
df$class.size[df$STR>=17.75 & df$STR<19.27]="medium-low"
df$class.size[df$STR>=19.27& df$STR<20.60]="medium-high"
df$class.size[df$STR>=20.60]="high"

reg=lm(test.score ~ class.size, df)
coeftest(reg, vcov. = vcovHC, type="HC1")
#If the str can not predict test.score, then the coeffients should all be 0. 
#H0:"class.sizelow=0","class.sizemedium-high=0","class.sizemedium-low=0"
linearHypothesis(reg, c("class.sizelow=0","class.sizemedium-high=0","class.sizemedium-low=0"), white.adjust = "hc1")
#p_value=0.003001 <0.01
```
As we can see from the result, The joint test of the three class-size coefficients strongly rejects the null hypothesis (p=0.003), indicating that class size categories have explanatory power for student test scores.

### Combination of categorical variables and continuous varibles in regressions

```{r}
#Question 5
Y=df$test.score
X1=df$STR
X2=(df$class.size=="medium-low")*1
X3=(df$class.size=="medium-high")*1
X4=(df$class.size=="high")*1

reg=lm(Y~X1+X2+X3+X4+X1:X2+X1:X3+X1:X4)
result=coeftest(reg, vcov. = vcovHC, type="HC1")
result
#if the class.size=low, X2=0, X3=0, X4=0, X1:X2=0,X1:X3=0,X1:X4=0. 
#The effect of STR in schools with low class size is beta1
m=2.58
beta1.bar=result[2,1]
beta1.bar
#-0.9914103
SE_beta1=result[2,2]
lower_bound=beta1.bar-m*SE_beta1
Upper_bound=beta1.bar+m*SE_beta1
lower_bound #-8.174011
Upper_bound #6.19119

```


